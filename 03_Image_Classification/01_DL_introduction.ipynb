{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Classification: Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Deep Learning is the specific area of Machine Learning using Neural Networks.\n",
    "\n",
    "Deep Learning has got some momentum thanks to the Big Data: a Deep Learning model usually needs more data than a regular Machine Learning one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are several types of Deep Learning architectures:\n",
    "* Regular Neural Networks (or MLP)\n",
    "* Convolutional Neural Networks\n",
    "* Recurrent Neural Networks\n",
    "* Generative Adversarial Networks\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.1. Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The perceptron, which is the fundamental basis of almost all Deep Learning, was first proposed by Frank Rosenblatt, a Psychologist, in 1957.\n",
    "\n",
    "He found his inspiration in the neuron:\n",
    "\n",
    "![](images/neuron-illustration.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/Perceptron_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.2. Neural Network representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neural networks are usually represented on diagrams using a standard convention:\n",
    "![](images/MLP_with_activations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This diagram has to be read from left to right.\n",
    "\n",
    "On the left, here in red, is the **input layer**: this is actually the input features $X=(x_1, x_2, x_3)$ (e.g. number of rooms in a house, presence of a garden...). The number of **units** is the number of input features.\n",
    "\n",
    "On the right, in green, is the **output layer**: this is the prediction of target value (e.g. the house price in a regression, or the class in classification). The number of **units** depends on the task (for regression it is usually one, for multiclass classification the number of classes).\n",
    "\n",
    "There is always one and only one input layer and output layer.\n",
    "\n",
    "In the middle, in blue, are the **hidden layers**. There can be an arbitrary number of hidden layers. The hiddens layers also have an arbitrary number of **units**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.3. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we understand the representation, we would understand how to compute the output predicted value, given input features $X = (x_1, x_2, ..., x_N)$.\n",
    "\n",
    "To compute this output predicted value, we need to weights $W$ of each unit.\n",
    "\n",
    "Each unit $i$ of a layer $l$ (except the input layer) has associated weights $W^{[l]}_i$. We will use those weights compute the activation $a^{[l]}_i$ of each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Considering our example in diagram, we could compute the activations of the first hidden layer using the following formulas:\n",
    "\n",
    "$$\n",
    "a^{[1]}_{1} = W^{[1]}_{1} \\times X + b^{[1]}_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[1]}_{2} = W^{[1]}_{2} \\times X + b^{[1]}_{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[1]}_{3} = W^{[1]}_{3} \\times X + b^{[1]}_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[1]}_{4} = W^{[1]}_{4} \\times X + b^{[1]}_{4}\n",
    "$$\n",
    "\n",
    "Where $b^{[1]}_{i}$ is called the bias, and i just an additional parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now if we want to compute the activations of the second hidden layer $a^{[2]}_i$, we would use the exact same formulas, but with the activations of the first hidden layer as input ($a^{[1]}_i$), instead of the input features:\n",
    "\n",
    "$$\n",
    "a^{[2]}_{1} = W^{[2]}_{1} \\times a^{[1]} + b^{[2]}_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[2]}_{2} = W^{[2]}_{2} \\times a^{[1]} + b^{[2]}_{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[2]}_{3} = W^{[2]}_{3} \\times a^{[1]} + b^{[2]}_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[2]}_{4} = W^{[2]}_{4} \\times a^{[1]} + b^{[2]}_{4}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, to compute the output layer would be exactly the same:\n",
    "$$\n",
    "a^{[3]}_{1} = W^{[3]}_{1} \\times a^{[2]} + b^{[3]}_{1}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.4. Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Neural networks, usually there is one more step: the activation function, often called $g$.\n",
    "\n",
    "Indeed, if we use the above formulas, the activation values $a^{[l]}_i$ have no boundaries: they could go up to Â± infinity. Moreover, though less intuitive, activation functions add non-linearity to the MLP: without activation function, only linearly separable problems can be solved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/activation_purpose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the new formulas are almost the same:\n",
    "\n",
    "$$\n",
    "a^{[1]}_{1} = g(W^{[1]}_{1} \\times X + b^{[1]}_{1}) = g(z^{[1]}_{1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[1]}_{2} = g(W^{[1]}_{2} \\times X + b^{[1]}_{2}) = g(z^{[1]}_{2})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[1]}_{3} = g(W^{[1]}_{3} \\times X + b^{[1]}_{3}) = g(z^{[1]}_{3})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[1]}_{4} = g(W^{[1]}_{4} \\times X + b^{[1]}_{4}) = g(z^{[1]}_{4})\n",
    "$$\n",
    "\n",
    "And so on for the other layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.5. The main activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are five main activation functions: Sigmoid, Tanh, Softmax, Relu and Linear. We will make a review of them now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sigmoid** is a historic activation function, but not the most efficient in general, now used mainly in last layer of a binary classification. The formula is the following:\n",
    "\n",
    "$$ sigmoid(x) = \\frac{1}{1+e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Hyperbolic tangent (or tanh)** is quite similar to sigmoid function. The shape is close to the one of sigmoid, but ranges between -1 and 1, while sigmoid ranges between 0 and 1. This is has a symmetry that sigmoid does not.\n",
    "This is not the most frequently used activation function at first approach anymore.\n",
    "\n",
    "The formula of tanh is the following:\n",
    "$$\n",
    "tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Rectified linear unit (or relu)** is really different than sigmoid and tanh activations. Even though it looks odd at first sight, it has the advantage of avoiding the problem of vanishing gradient. It is currently one of the most widely used activation function.\n",
    "\n",
    "The formula is the following:\n",
    "$$\n",
    "relu(x) = max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-2, 2, 0.02)\n",
    "sigmoid = 1./(1+np.exp(-x))\n",
    "tanh = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "relu = np.max([np.zeros(len(x)), x], axis=0)\n",
    "plt.plot(x, sigmoid)\n",
    "plt.plot(x, tanh)\n",
    "plt.plot(x, relu)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('activation')\n",
    "plt.legend(['sigmoid', 'tanh', 'relu'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Softmax** function is a special activation function, used in only one specific case: the last layer of a multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Linear** function is just the identity, and is used mainly in the specific case of last layer of a regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## II.1 Image and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Images are just numbers! So all we learnt about Machine Learning has to work with images, both classification and regression!\n",
    "![](images/computer_vision_task.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But unlike other kind of data, images have **spatial structure**! Indeed, the same object can be seen from various viewpoints, deformed, scaled, occluded, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/Computer_Vision_limits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That's why in order to take full advantage of this spatial structure, we need in **add spatial information in our models**. This is where Convolutional Neural Networks (CNN) take place! So first, let's talk a bit about image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.2 Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Paddind an image is adding borders to it, thus increasing the size of the image. Most of the time, images are padded with zeros.\n",
    "\n",
    "Below is an example of a 4x4 image before padding, and after padding:\n",
    "\n",
    "![](images/padding_image.png)\n",
    "\n",
    "Padding can also add more than one line of zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.3 Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A kernel in image processing is nothing more than a small image (or a small matrix), that we will use to make a convolution.\n",
    "\n",
    "Thus, a kernel is necessary smaller than the image. Usually, kernels have odd dimensions, and are quite small. Typical sizes of kernels are:\n",
    "* 3x3\n",
    "* 5x5\n",
    "* 7x7\n",
    "* 9x9\n",
    "\n",
    "Depending on the values of the kernel, the convolution will provide different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.4 Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A convolution is a mathematical operation that you already encountered. For example, below is the convolution result of an image with a 3x3 kernel of ones:\n",
    "\n",
    "![](images/convolution_animated.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But as you know, the kernel can have different values. Below is an example of a convolution of an image I with a non uniform kernel K:\n",
    "\n",
    "![](images/convolution_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A wisely chosen kernel can be really useful to detect patterns in images. Here is a list of commonly used kernels and what they do on a given image (from [wikipedia](https://en.wikipedia.org/wiki/Kernel_(image_processing)):\n",
    "\n",
    "| Kernel | Usage | Example |\n",
    "|:--:|:--:|:--:|\n",
    "| \\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}| Identity | ![](images/Vd-Orig.png)  |\n",
    "|\\begin{bmatrix}\n",
    "\\ \\ 1 & 0 & -1 \\\\\n",
    "\\ \\ 0 & 0 & \\ \\ 0 \\\\\n",
    "-1 & 0 & \\ \\ 1\n",
    "\\end{bmatrix} | Edge detection | ![](images/Vd-Edge1.png)  |\n",
    "| \\begin{bmatrix}\n",
    "0 &  \\ \\ 1 & 0 \\\\\n",
    "1 & -4 & 1 \\\\\n",
    "0 &  \\ \\ 1 & 0\n",
    "\\end{bmatrix}| Edge detection | ![](images/Vd-Edge2.png)  |\n",
    "| \\begin{bmatrix}\n",
    "-1 &  -1 & -1 \\\\\n",
    "-1 & \\ \\ 8 & -1 \\\\\n",
    "-1 &  -1 & -1\n",
    "\\end{bmatrix}| Edge detection | ![](images/Vd-Edge3.png)  |\n",
    "| \\begin{bmatrix}\n",
    "\\ \\ 0 & -1 & \\ \\ 0 \\\\\n",
    "-1 & \\ \\ 5 & -1 \\\\\n",
    "\\ \\ 0 & -1 & \\ \\ 0\n",
    "\\end{bmatrix}| Sharpening | ![](images/Vd-Sharp.png)  |\n",
    "| \\begin{bmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "2 & 4 & 2 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}| Gaussian blur | ![](images/Vd-Blur1.png)  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So to summarize, a convolution is a mathematical operation on a image with a given kernel, resulting in a new processed image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.5 Strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Up to now, we know how to compute a convolution, with a given kernel, and to pad an image. The last tool needed is what is called stride.\n",
    "\n",
    "The stride value is the step between two convolutions with a kernel. For the moment, all we did was with a stride of one, as in the example below:\n",
    "\n",
    "![](images/Stride1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So as we see in this example, an image of dimension 7x7, convoluted with a kernel of 3x3 results in an image of 5x5.\n",
    "\n",
    "What if we use a stride of 2, meaning our kernel with use a step of 2 on our image:\n",
    "\n",
    "![](images/Stride2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With a stride of 2, an input image of dimension 7x7 convoluted with a kernel of 3x3 results in an image of 3x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# III. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## III.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why would we use convolutions in neural networks? Isn't a MLP complicated enough?\n",
    "\n",
    "Well, indeed MLP are complicated enough, but they do not take into account *spatial* correlations. Indeed in an image, there are spatial correlations everywhere:\n",
    "* In a face the eye is always close to the nose, and the nose is always in the middle of the face.\n",
    "* A bike always has two wheels.\n",
    "* All numbers and letters have a particular shape.\n",
    "\n",
    "For all of those examples, a classical MLP will not see the spatial correlations, and might have to learn them by itself. But if you add convolutions, this is far more easier to understand the structure of the data for the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/CNN_features_levels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## III.2 Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To make a CNN, we need first to make a Convolutional Layer. A convolutional layer is simply a convolution on our input data, with several options to choose: the size of the kernel, the padding and the stride. \n",
    "\n",
    "Here is the signature in TensorFlow:\n",
    "\n",
    "```python\n",
    "tensorflow.keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With especially the following parameters to use:\n",
    "* `filters`: the number of kernels to use\n",
    "* `kernel_size`: the size of the kernels (e.g. `(3,3)` for a 3x3 kernel)\n",
    "* `padding`: `valid` for no padding, `same` to keep the same dimensions for the image\n",
    "* `kernel_regularizer`: to add regularization\n",
    "\n",
    "We will see how to use it in an example in the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## III.3 Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A pooling layer is a layer that will reduce the size of the images by taking the average or max value of a given number of pixels.\n",
    "\n",
    "Below is an example of pooling layer, with a kernel of 2x2 and a stride of 2 applied on an image of 4x4:\n",
    "\n",
    "![](images/max_pooling.jpeg)\n",
    "\n",
    "A pooling layer is usually used right after a convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pooling layers are defined in Keras as well, with the following signature:\n",
    "\n",
    "`tensorflow.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "Where:\n",
    "* `pool_size` is the dimensions of the kernel\n",
    "* `strides` is the stride value, if `None` the value will be the same as `pool_size`\n",
    "* `padding` can be `valid` for no padding or `same` for keeping image dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# IV. Application: LeNet-5 on MNIST digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## IV.1 Architecture diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will first see the architecture of the LeNet-5: this is the algorithm that was proposed by [LeCun et al.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) for digit classification using convolutional networks.\n",
    "\n",
    "![](images/Lenet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will now decompose and explain the layers step by step:\n",
    "* Input (32x32): this is the input image of a digit\n",
    "* C1 (6@28x28): this is a convolutional layer of 3x3 with 6 filters\n",
    "* S2 (6@14x14): this is a max pooling layer of 2x2\n",
    "* C3 (16@10x10): this is a convolution layer of 3x3 with 10 filters\n",
    "* S4 (16@5x5): this is a max pooling layer of 2x2\n",
    "* C5 (120): this is a fully connected layer (MLP) of 120 units\n",
    "* F6 (84): this is a fully connected layer (MLP) pf 84 units\n",
    "* Output (10): this is a softmax layer of 10 units (1 units per digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IV.2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The implementation of the LeNet-5 algorithm would be the following in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D, Conv2D, Flatten, Dense\n",
    "\n",
    "\n",
    "def lenet5():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer C1\n",
    "    model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    # Layer S2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer C3\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "    # Layer S4\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Before going into layer C5, we flatten our units\n",
    "    model.add(Flatten())\n",
    "    # Layer C5\n",
    "    model.add(Dense(units=120, activation='relu'))\n",
    "    # Layer F6\n",
    "    model.add(Dense(units=84, activation='relu'))\n",
    "    # Output layer\n",
    "    model.add(Dense(units=10, activation = 'softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You will soon get familiar with the classes of TensorFlow/Keras. Here we are using the following:\n",
    "- `Sequential()`: this will contain all the layers of our neural network\n",
    "- `Dense(units, activation=None)`: this is a classical multi layer perceptron, taking as input the number of units (i.e. Neurons) and the activation function\n",
    "- `Conv2D(filters, kernel_size, activation)`: this is the convolutional layer\n",
    "- `MaxPooling2D(pool_size)`: this is the pooling layer\n",
    "- `Flatten()`: this is a layer that allows to go from 2D information to 1D information\n",
    "\n",
    "> **NB:** the first layer of a `Sequential` model always takes as input the `input_shape`: this is the `shape` of your images! Otherwise TensorFlow does not know how many input features you have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, one can review a model with the `summary()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 16)        880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 60,074\n",
      "Trainable params: 60,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = lenet5()\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IV.3 Model compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then you have to compile your model: indeed TensorFlow does not fully work on Python, so a compilation step is necessary. In the compilation, you can provide several parameters.\n",
    "* The optimizer is the optimization algorithm: you already know the gradient descent, you can call it by using `optimizer='adam`\n",
    "* The loss function: for binary classification use `loss='binary_crossentropy'`, multiclass classification `loss='categorical_crossentropy'`, for regression use `loss='mean_squared_error'`\n",
    "* You can also play with the metrics to display in real time, for example to display the accuracy, add the parameter `metrics=['accuracy']`\n",
    "\n",
    "For more information, the documentation is [here](https://keras.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IV.4 Model training: Application to MNIST Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will now apply this model to the MNIST digits classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "# Import the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Rescale the data\n",
    "X_train = X_train/255.\n",
    "X_test = X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# Transform the targets to categorical vectors\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, you want to fit your model: just like in scikit-learn! \n",
    "\n",
    "Some parameters have to be given:\n",
    "* `x`: the input features\n",
    "* `y`: the labels or target values\n",
    "* `epochs`: the number of times you iterate over all the input samples\n",
    "* `batch_size`: the number of samples used before updating the parameters of the model (we will speak more about it tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 15s 250us/sample - loss: 0.2341 - accuracy: 0.9311 - val_loss: 0.0824 - val_accuracy: 0.9746\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 16s 266us/sample - loss: 0.0748 - accuracy: 0.9763 - val_loss: 0.0682 - val_accuracy: 0.9775\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 17s 288us/sample - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9861\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 0.0415 - accuracy: 0.9870 - val_loss: 0.0537 - val_accuracy: 0.9823\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 13s 219us/sample - loss: 0.0332 - accuracy: 0.9890 - val_loss: 0.0430 - val_accuracy: 0.9861\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0302 - accuracy: 0.9903 - val_loss: 0.0385 - val_accuracy: 0.9851\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 13s 216us/sample - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0372 - val_accuracy: 0.9880\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.0428 - val_accuracy: 0.9868\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 13s 221us/sample - loss: 0.0164 - accuracy: 0.9948 - val_loss: 0.0435 - val_accuracy: 0.9866\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0168 - accuracy: 0.9947 - val_loss: 0.0350 - val_accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd9403f5c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally fit the model\n",
    "my_model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IV.5. Prediction and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To predict, do not change your habits: use the function `.predict(X)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.5133186e-11, 3.6845185e-07, 4.6304319e-10, 5.5415690e-08,\n",
       "        1.5886305e-07, 5.1210947e-09, 4.6747328e-13, 9.9999952e-01,\n",
       "        1.1317307e-08, 1.7834157e-08],\n",
       "       [1.4172390e-12, 1.0881038e-11, 1.0000000e+00, 3.4740478e-13,\n",
       "        2.1218947e-12, 2.1520169e-18, 7.1480209e-14, 2.7457800e-11,\n",
       "        2.3750067e-14, 1.3164868e-16],\n",
       "       [5.1503086e-09, 9.9999642e-01, 5.4470078e-08, 6.3477023e-12,\n",
       "        1.2005552e-06, 9.6305339e-11, 1.7442122e-08, 2.2505990e-06,\n",
       "        5.5228011e-08, 6.3406058e-10],\n",
       "       [9.9924004e-01, 2.5692966e-09, 1.1095599e-06, 1.8062543e-09,\n",
       "        2.1377916e-07, 8.2750194e-07, 7.4763672e-04, 1.2615376e-07,\n",
       "        1.2306541e-06, 8.8048027e-06],\n",
       "       [1.4827008e-10, 3.8723260e-07, 2.4772440e-09, 2.8537400e-08,\n",
       "        9.9992085e-01, 3.3212810e-08, 2.9100583e-08, 4.0229605e-08,\n",
       "        3.4418392e-07, 7.8278812e-05]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict for some values\n",
    "my_model.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To evaluate, you can of course use scikit-learn's metrics as usual. But warning, the keras `.predict()` method does not return classes, but probabilities!\n",
    "\n",
    "You can also use `.evaluate(X, y)`:\n",
    "This function returns a list with the loss and the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is: 0.034982868475666326\n",
      "accuracy is: 0.9883\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model\n",
    "loss, accuracy = my_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('loss is:', loss)\n",
    "print('accuracy is:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our algorithm has about 99% accuracy. Amazing, right?!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
