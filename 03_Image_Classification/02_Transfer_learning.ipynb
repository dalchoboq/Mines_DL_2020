{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmBlupy1fUme",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Classification: Computer Vision and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F68UAmNSfUmi",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# I. Computer Vision: Deep Learning Era"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NG9r8osjfUmk",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## I.1 AlexNet: the game changer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUwWNmumfUmo",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Historically, convolutional neural networks were not always famous. Yann Le Cun, commonly admitted as one of the creators of this technique, is now a rock star in AI, but had rough years in academic research in the past.\n",
    "\n",
    "CNNs really exploded with [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) in 2012, a CNN that made a breakthrough in the Computer Vision field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qn7vvujv-OfS",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/image_net_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOGN7yv9fUmp",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The architecture of AlexNet is the following:\n",
    "\n",
    "![](images/AlexNet-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bub54eZfUmr",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The architecture is the following:\n",
    "* Convolutional layer of 11x11 with 96 filters and stride of 4\n",
    "* Max pooling layer of 3x3 with stride of 2\n",
    "* Convolutional layer of 5x5 with 256 filters\n",
    "* Max pooling layer of 3x3 with stride of 2\n",
    "* Convolutional layer of 3x3 with 384 filters\n",
    "* Convolutional layer of 3x3 with 384 filters\n",
    "* Convolutional layer of 3x3 with 256 filters\n",
    "* Max pooling layer of 3x3 with stride of 2\n",
    "* Fully connected layer of 4096 units\n",
    "* Fully connected layer of 4096 units\n",
    "* Output: fully connected softmax layer of 1000 units (1 units per class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pc-1m0x8fUms",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since then, CNNs are quite popular and widely used in the Computer Vision: many new powerful architectures have been proposed. Let's see some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAtejPoLfUmu",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.2 VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9k6UHp-fUmv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "VGG is one the best architectures of the 2014 image net challenge. It has 16 layers, with the following architecture:\n",
    "\n",
    "![](images/VGG16_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEUAhI3fUmy",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you want to reuse the VGG architecture, it has been made very user friendly within Keras.\n",
    "\n",
    "The object VGG16 exists with the following signature:\n",
    "```python\n",
    "keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "```\n",
    "\n",
    "So that one can define a VGG16 architecture with just one line of code.\n",
    "\n",
    "Even better, you can reuse the weights of the network trained on the image net challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SkX7dupZfUmz",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.3 ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQZu1ruAfUm1",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ResNet (for Residual Network) is the winner of the 2015 image net challenge.\n",
    "\n",
    "It was a mini revolution, introducing for the first time the residual blocks:\n",
    "\n",
    "![](images/residual_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMaUxRN5fUm2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Residual blocks are just skip connections: the network can choose to skip one or more layers, if it's better for the learning to do so.\n",
    "\n",
    "The main advantage is that it help learning over very deep networks: at worst, nothing is learnt, but the information keep going forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyBu6LNVfUm3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There were several versions of the ResNet, with several depth: 50, 101 and 152 layers. All are available in Keras for easy implementation:\n",
    "```python\n",
    "keras.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "keras.applications.resnet.ResNet101(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "keras.applications.resnet.ResNet152(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "```\n",
    "\n",
    "Again, they are available with weights trained on the image net challenge, which is really convenient for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "keUfnCRmfUm4",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.4 Other architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BGNvwHyfUm5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are plenty of other architectures that are quite famous, and that can be used. Many of them are available in Keras [here](https://keras.io/applications/):\n",
    "- Inception\n",
    "- MobileNet\n",
    "- DenseNet\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFzMI6rifUm6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. Neural Networks Optimization & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BPULmbsfUm7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## II.1 Epochs and Minibatch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID6O84gPfUm8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The number of epochs is the number of time all samples are used to train your neural network. Basically, the more you have epochs, the more your neural network is trained on your data and knows it well.\n",
    "\n",
    "The minibatch size is the number of samples used to update the parameters (weights and bias) of your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "flgDlzeafUm-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below is a representation of the convergence of an gradient descent for three extreme sizes of minibatch:\n",
    "* stochastic means minibatch size = 1\n",
    "* batch means minibatch size is the all the samples\n",
    "* minibatch is in between\n",
    "\n",
    "Each point represent an update of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WuuxHhKfUm_",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/minibatch_vs_batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Ci2KRrrfUnA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize this:\n",
    "- Stochastic might be noisy\n",
    "- Batch can take longer to reach minimum\n",
    "- Mini-batch is a tradeoff: a bit noisy but quite fast to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7XxJYLofUnC",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To sum up, it is recommended to use minibatch gradient descent, with typical minibatch size between 32 and 256. This is chosen when fitting a model with the paramter `batch_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDz9BW95fUnD",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.2 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7D_UP7FcfUnE",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Would not recommend to play with this hyperparameter at first, but you can try to fine tune it to achieve the best possible results. The learning rate should allow you to reach the global minimum on such a curve:\n",
    "\n",
    "![](images/loss_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6y1gdl--Of_",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can imagine, there are three possibilities:\n",
    "- A very small learning rate: you might be stuck in the local minimum\n",
    "- A very large learning rate: you might overshoot the global minimum\n",
    "- A reasonable learning rate: you will endup in the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3JcBi0hfUnG",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following plot shows the potentiel impact of the learning rate on the loss (and thus the accuracy of your model).\n",
    "\n",
    "![](images/lr_epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beMBht8FfUnH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is just a schematic view, but when a very fine tuning is required, playing with the learning rate can be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jLP5GxKfUnI",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.3 Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNkjiMTNfUnJ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Several optimizers exist. Up to now, we only used the gradient descent (`SGD` in keras). But many other optimizers exist. The most frequently used are the following:\n",
    "* `SGD` for Stochastic Gradient Descent\n",
    "* `RMSprop` for Root Mean Square propagation\n",
    "* `adam` for Adaptive Moment optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxrWVthefUnL",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adam propagation tends to converge in less epochs and to be deal better with noise:\n",
    "![](images/optimizers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oa3urmRfUnN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is recommended to use `adam` with default parameters in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFjr6s3ifUnQ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.4 Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1SkFCiyfUnS",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neural networks, as you know, are a really powerful tool. With a complex enough hierarchy, a neural network can learn almost anything. But the drawback is that they can overfit really easily.\n",
    "\n",
    "Then, more than for any other model, regularization might be key to a good model.\n",
    "\n",
    "Below is a plot of the loss of a neural network as a function of the number of epochs, trained on the MNIST digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbxJfJJ2fUnT",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/mnist_train_and_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D16gxgtBfUnU",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As one can see, on this example, after 10 epochs the model is actually overfitting and the test set loss does not decrease anymore. Even worse: the test loss increases!\n",
    "\n",
    "> This means after 10 epochs the **performance** of the MLP is **decreasing**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0bLBDmHfUnV",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overfitting usually does not happen during the first epochs. Then a method so called **early stopping** is to **stop training before the network overfits**.\n",
    "\n",
    "Basically, in this example, a good early stopping would have been to stop around epoch 10.\n",
    "\n",
    "However, TensorFlow allows to automatize that process, using the following callback method: \n",
    "\n",
    "```Python\n",
    "tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5IsMbUpjfUnW",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.5 Layers and Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nT4V620GfUnX",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Playing with layers and units is another way to add regularization. Indeed, overfitting is usually cause by a model too complex for the task. Thus, sometimes reducing the number of layers and units might decrease overfitting and then add regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryed_ObmfUnY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.6 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puto2VkYfUnZ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A commonly used way of regularizing is the **dropout**. Dropout randomly sets some activation functions to 0 while fitting: this is equivalent to **randomly remove some units** of the neural network.\n",
    "\n",
    "Why? Because it forces the network to not rely on some units only, but to use them all! Thus is may prevent from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xjW--TDAfUna",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGl0zS0ifUnb",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This can be used in TensorFlow adding a `Dropout` layer, with `p` specifying the probability (between 0 and 1) of a unit to be set to 0:\n",
    "```Python\n",
    "tf.keras.layers.Dropout(p=0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcspniPlfUnc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.7 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "09NxOVi_fUnd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In some cases, if your model overfits or does not work properly, this is just because you don't have enough samples in your training set! Indeed, Deep Learning models need a lot of data to work properly.\n",
    "\n",
    "Thus sometimes you can then use data augmentation, which is the **artificial augmentation of your training dataset**. One of the most common example of data augmentation is in Computer Vision. You can modify an image to increase your training size with the following operations:\n",
    "* Mirroring\n",
    "* Croping\n",
    "* Rotating\n",
    "* Zooming\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1ikpZd_-Oga",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below is an example of original cat picture (on left) and six artificially created images from the original. That way, you have artificially 7 times more data!\n",
    "\n",
    "![](images/cat_augmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FEswSj4dfUne",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To do such thing, TensorFlow has a really powerful built-in function with the following signature:\n",
    "`tf.keras.preprocessing.image.ImageDataGenerator`\n",
    "\n",
    "You can find more on its usage [here](https://keras.io/preprocessing/image/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAJG2as8fUne",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# III. Application with Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGEy3maffUnf",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## III.1. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwP4x0osfUng",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Transfer Learning is the ability of a model to reuse knowledge from a given problem, to another problem.\n",
    "\n",
    "For example, you want to train a model to classify cars, bus, trucks... on pictures, but you don't have much pictures of cars: how would you do?\n",
    "\n",
    "You can reuse a model that was trained for autonomous driving, that is used to 'see' this kind of objects, and retrain it.\n",
    "\n",
    "Just like we humans, can recognize almost any kind of cats easily, because are already used to 'see' cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrl1ujT1fUnh",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is a really powerful method, that can lead to improved performance and/or fast training.\n",
    "\n",
    "![](images/transfer_learning_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ymxh7kpyfUni",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## III.2 CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sIdZ7DonfUnk",
    "outputId": "2e77f563-7ea2-4374-aaa8-32231b73b174",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# If needed, take only 10000 images for time and memory reasons\n",
    "#X_train = X_train[:10000]\n",
    "#y_train = y_train[:10000]\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "6WZXeq5dfUnp",
    "outputId": "300194fa-11f7-4208-d54d-9a156b047903",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'frogs')"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label = ['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks']\n",
    "\n",
    "idx = 0\n",
    "\n",
    "plt.imshow(X_train[idx])\n",
    "plt.title(label[y_train[idx][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcNBe6RIfUnt",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PoEclgyKfUnu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfNOWPjnfUn1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## III.3 Model training from scratch\n",
    "\n",
    "We will try to build a VGG model and train it from scratch, and then evaluate the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcBCCRBVmgyk",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# create the base model\n",
    "base_model = VGG16(weights=None, include_top=False, \n",
    "             input_shape=X_train.shape[1:])\n",
    "\n",
    "# add a flattening layer\n",
    "# let's add a fully-connected layer to classify\n",
    "x = base_model.get_layer('block3_pool').output\n",
    "x = GlobalAveragePooling2D(name='GlobalPooling')(x)\n",
    "x = Dense(512, activation='relu', name='fc1')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "id": "q_W8ND0w-OhE",
    "outputId": "40bec108-d983-49b8-8b0f-53afac72784c",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "GlobalPooling (GlobalAverage (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,872,202\n",
      "Trainable params: 1,872,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMDTCn1XfUn6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can add some `callbacks`, here `EarlyStopping` and `TensorBoard`.\n",
    "\n",
    "TensorBoard allows to have a follow up of the training: the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JH77TI3HfUn8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Define now our callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             TensorBoard(log_dir='./Graph')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66Rm8ydSfUn_",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's compile and fit our model on our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "bA3obitKfUoA",
    "outputId": "4361d0d5-eb9c-459c-e9cb-af8a9f1199fd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/98 [..............................] - ETA: 0s - loss: 2.3025 - accuracy: 0.1211WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/98 [..............................] - ETA: 5s - loss: 3.6627 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0263s vs `on_train_batch_end` time: 0.0787s). Check your callbacks.\n",
      "98/98 [==============================] - 6s 66ms/step - loss: 2.3905 - accuracy: 0.1377 - val_loss: 2.4415 - val_accuracy: 0.1011\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 2.1223 - accuracy: 0.1991 - val_loss: 1.9851 - val_accuracy: 0.2361\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 1.9830 - accuracy: 0.2589 - val_loss: 1.9294 - val_accuracy: 0.2714\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 1.8078 - accuracy: 0.3321 - val_loss: 1.6857 - val_accuracy: 0.3597\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 1.6733 - accuracy: 0.3854 - val_loss: 1.5685 - val_accuracy: 0.4199\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 1.5602 - accuracy: 0.4305 - val_loss: 1.3703 - val_accuracy: 0.4946\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 6s 59ms/step - loss: 1.4417 - accuracy: 0.4741 - val_loss: 1.3519 - val_accuracy: 0.5177\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 1.3464 - accuracy: 0.5197 - val_loss: 1.3630 - val_accuracy: 0.4972\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 1.2412 - accuracy: 0.5569 - val_loss: 1.0667 - val_accuracy: 0.6225\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 1.1418 - accuracy: 0.5961 - val_loss: 1.1473 - val_accuracy: 0.5946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5a82100048>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=512, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uGoByVrfUoG",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After 10 epochs and a lot of computational time, we have reached an accuracy of about 60 % on the validation dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iqIoyObfUoI",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then to use Tensorboard, just go back to your terminal and type\n",
    "`tensorboard --logdir=Graph` where `Graph` is the path you defined in the tensorboard callback. You will see something like this:\n",
    "\n",
    "![](images/tensorboardoutput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnsGz4k4fUoJ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## III.4 Model training with transfer learning\n",
    "\n",
    "Now let's redo the same, but using transfer learning, with the pretrained network on the imagenet competition, using `weights='imagenet'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "HWj0dwkoAY6J",
    "outputId": "e76a5cdb-346f-47f7-c5b3-a119e04f398f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2/98 [..............................] - ETA: 6s - loss: 205.2600 - accuracy: 0.1221WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0227s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 6.7953 - accuracy: 0.1715 - val_loss: 2.0249 - val_accuracy: 0.2494\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 2.2348 - accuracy: 0.2229 - val_loss: 2.0523 - val_accuracy: 0.3092\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 1.8036 - accuracy: 0.3558 - val_loss: 1.8525 - val_accuracy: 0.3165\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 1.5742 - accuracy: 0.4533 - val_loss: 1.1547 - val_accuracy: 0.5951\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 1.2800 - accuracy: 0.5527 - val_loss: 1.1245 - val_accuracy: 0.6059\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 1.0722 - accuracy: 0.6292 - val_loss: 1.0688 - val_accuracy: 0.6307\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 0.9418 - accuracy: 0.6746 - val_loss: 0.7678 - val_accuracy: 0.7388\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 0.7855 - accuracy: 0.7299 - val_loss: 0.8413 - val_accuracy: 0.7241\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 0.7005 - accuracy: 0.7572 - val_loss: 0.8761 - val_accuracy: 0.7091\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 6s 58ms/step - loss: 0.5923 - accuracy: 0.7942 - val_loss: 0.7032 - val_accuracy: 0.7705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5a063fed30>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the base model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, \n",
    "             input_shape=X_train.shape[1:])\n",
    "\n",
    "# add a flattening layer\n",
    "# let's add a fully-connected layer to classify\n",
    "x = base_model.get_layer('block3_pool').output\n",
    "x = GlobalAveragePooling2D(name='GlobalPooling')(x)\n",
    "x = Dense(512, activation='relu', name='fc1')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=512, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nvgiNPxDC4M",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The accuracy jumped up to 77 %, quite an improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gkYEA1W-Ohf",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally you can try other famous DL architectures, and perhaps improve your performances."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "02_Transfer_learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
