{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digits Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this challenge, you will use classification methods to discriminate images of handwritten digits.\n",
    "\n",
    "Back in the 90's, it was a great machine learning challenge.\n",
    "\n",
    "In this challenge, you will be a bit guided (if needed), ad use PCA and classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first: download the dataset (or actually a simplified subsample of the original dataset) using scikit-learn. The data is available in `datasets.load_digits()`.\n",
    "\n",
    "Then split the data into `X` and `y`: the data and the associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the MNIST dataset and split it into X and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play with the data: understand at least its mains characteristics:\n",
    "- the number of samples\n",
    "- the number of classes\n",
    "- the dynamics of the values (min and max values)\n",
    "- the shape of the data\n",
    "- any other things you find relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will display a random handwritter digit image, as well as its associated label.\n",
    "To do so, you might need to use:\n",
    "- `reshape` method of numpy\n",
    "- `plt.imshow()` method of matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reshape a sample and use imshow to display the digit image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now reduce the dimensionality of our data by applying PCA on it.\n",
    "\n",
    "First, a short reminder or intro on PCA:\n",
    "\n",
    "- what is a PCA\n",
    "- warning: needs centered data\n",
    "- How to implement it\n",
    "- How to use it with a number of components, or with a retained variance...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA stands for Principal Component Analysis. It allows to project data onto a given number of axis, in order to minimize the variance:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1-H7rbHT-qvci1ckOi-CE97cqudY9jJcS\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be used properly, the data needs to be zero-centered. Basically, you can rescale the data before using PCA, just like most of the machine learning models.\n",
    "\n",
    "The scikit-learn signature of the PCA is the following:\n",
    "```python\n",
    "class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0, iterated_power=’auto’, random_state=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the documentation, especially the use of the `n_components` parameter:\n",
    "- if a positive integer, it will be the number of principal components to keep\n",
    "- if a float between 0 and 1, it will be the minimum retained variance to keep\n",
    "\n",
    "After applying PCA (e.g. with `fit_transform()`), one can check the retained variance with the attribute `.explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply PCA to project your data on 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform PCA on the data with 2 retained dimensions (after mean normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can display the results: display the PCA values on a scatter plot, with as color the label of the data. Compute and display the retained variance.\n",
    "\n",
    "Does the PCA help separate the classes? How will a linear classification model perform on such data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the results of the PCA and the retained variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about this retained variance? Is it enough for you?\n",
    "\n",
    "How many components shall we keep in order to have a 99% retained variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the number of components needed to retained 99% variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the output of the PCA to with 99% retained variance to perform classification, and evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Classify and evaluate a classification model (do not forget to split!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use gridsearch and other models to improve your results if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
